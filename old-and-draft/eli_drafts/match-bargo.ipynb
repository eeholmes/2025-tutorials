{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6cee8b7-c49a-4e7c-a15e-0b88f467b834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLATFORM_NUMBER</th>\n",
       "      <th>CYCLE_NUMBER</th>\n",
       "      <th>PRES</th>\n",
       "      <th>CHLA_surface</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>CHLA_QC</th>\n",
       "      <th>log10_CHLA_surface</th>\n",
       "      <th>min_pres</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>time</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1902383</td>\n",
       "      <td>78</td>\n",
       "      <td>19.889999</td>\n",
       "      <td>0.017404</td>\n",
       "      <td>23.0209</td>\n",
       "      <td>-53.2220</td>\n",
       "      <td>2024-03-02 16:13:24.002000128</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.759363</td>\n",
       "      <td>19.889999</td>\n",
       "      <td>23.0209</td>\n",
       "      <td>-53.2220</td>\n",
       "      <td>2024-03-02 16:13:24.002000128</td>\n",
       "      <td>bgc_na_202403.nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1902383</td>\n",
       "      <td>79</td>\n",
       "      <td>7.980000</td>\n",
       "      <td>0.011863</td>\n",
       "      <td>22.7279</td>\n",
       "      <td>-52.3932</td>\n",
       "      <td>2024-03-12 15:19:00.002000128</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.925805</td>\n",
       "      <td>7.980000</td>\n",
       "      <td>22.7279</td>\n",
       "      <td>-52.3932</td>\n",
       "      <td>2024-03-12 15:19:00.002000128</td>\n",
       "      <td>bgc_na_202403.nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1902383</td>\n",
       "      <td>80</td>\n",
       "      <td>11.850000</td>\n",
       "      <td>0.066477</td>\n",
       "      <td>23.2649</td>\n",
       "      <td>-52.0005</td>\n",
       "      <td>2024-03-22 15:28:33.002000128</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.177332</td>\n",
       "      <td>11.850000</td>\n",
       "      <td>23.2649</td>\n",
       "      <td>-52.0005</td>\n",
       "      <td>2024-03-22 15:28:33.002000128</td>\n",
       "      <td>bgc_na_202403.nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1902384</td>\n",
       "      <td>79</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>0.025275</td>\n",
       "      <td>20.0154</td>\n",
       "      <td>-42.7101</td>\n",
       "      <td>2024-03-08 20:58:18.002000128</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.597309</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>20.0154</td>\n",
       "      <td>-42.7101</td>\n",
       "      <td>2024-03-08 20:58:18.002000128</td>\n",
       "      <td>bgc_na_202403.nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1902384</td>\n",
       "      <td>80</td>\n",
       "      <td>2.140000</td>\n",
       "      <td>0.014453</td>\n",
       "      <td>20.3018</td>\n",
       "      <td>-42.5525</td>\n",
       "      <td>2024-03-18 20:31:41.002000128</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.840057</td>\n",
       "      <td>2.140000</td>\n",
       "      <td>20.3018</td>\n",
       "      <td>-42.5525</td>\n",
       "      <td>2024-03-18 20:31:41.002000128</td>\n",
       "      <td>bgc_na_202403.nc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PLATFORM_NUMBER  CYCLE_NUMBER       PRES  CHLA_surface  LATITUDE  \\\n",
       "0          1902383            78  19.889999      0.017404   23.0209   \n",
       "1          1902383            79   7.980000      0.011863   22.7279   \n",
       "2          1902383            80  11.850000      0.066477   23.2649   \n",
       "3          1902384            79   2.260000      0.025275   20.0154   \n",
       "4          1902384            80   2.140000      0.014453   20.3018   \n",
       "\n",
       "   LONGITUDE                          TIME  CHLA_QC  log10_CHLA_surface  \\\n",
       "0   -53.2220 2024-03-02 16:13:24.002000128        1           -1.759363   \n",
       "1   -52.3932 2024-03-12 15:19:00.002000128        1           -1.925805   \n",
       "2   -52.0005 2024-03-22 15:28:33.002000128        1           -1.177332   \n",
       "3   -42.7101 2024-03-08 20:58:18.002000128        1           -1.597309   \n",
       "4   -42.5525 2024-03-18 20:31:41.002000128        1           -1.840057   \n",
       "\n",
       "    min_pres      lat      lon                          time       source_file  \n",
       "0  19.889999  23.0209 -53.2220 2024-03-02 16:13:24.002000128  bgc_na_202403.nc  \n",
       "1   7.980000  22.7279 -52.3932 2024-03-12 15:19:00.002000128  bgc_na_202403.nc  \n",
       "2  11.850000  23.2649 -52.0005 2024-03-22 15:28:33.002000128  bgc_na_202403.nc  \n",
       "3   2.260000  20.0154 -42.7101 2024-03-08 20:58:18.002000128  bgc_na_202403.nc  \n",
       "4   2.140000  20.3018 -42.5525 2024-03-18 20:31:41.002000128  bgc_na_202403.nc  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"na_surface_chla.parquet\")\n",
    "date_str = sorted(df[\"time\"].dt.strftime(\"%Y-%m-%d\").unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "613c7d0f-9882-4348-bd80-a99238355443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "auth = earthaccess.login()\n",
    "# are we authenticated?\n",
    "if not auth.authenticated:\n",
    "    # ask for credentials and persist them in a .netrc file\n",
    "    auth.login(strategy=\"interactive\", persist=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c10855-0a5c-46d4-8320-d705e399cf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray as xr\n",
    "results = earthaccess.search_data(\n",
    "    short_name = \"PACE_OCI_L3M_RRS\",\n",
    "    temporal = (df.time.min(), df.time.max()),\n",
    "    granule_name=\"*.DAY.*.4km.nc\"\n",
    ")\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0070c049-2bfc-4f94-9e3b-3ec32ca8c866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9803fb3026d94432b4d7a3bc8a53ae71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d531e14fbfb6489e96c82368cb39d0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9652470c814396a888ffe5b4a78f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fileset = earthaccess.open(results);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f304e8ff-e2d9-44d9-8065-fa8f7037bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the netcdfs do not have time so we need to get that from the results metadata\n",
    "# Use the beginning of each 8-day window as the time coordinate\n",
    "import numpy as np\n",
    "fileset_time = [\n",
    "    np.datetime64(r[\"umm\"][\"TemporalExtent\"][\"RangeDateTime\"][\"BeginningDateTime\"][:10])\n",
    "    for r in results\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63fd77eb-26f8-499e-98c8-cf5ffd438c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "time_val = fileset_time[i]\n",
    "ds = xr.open_dataset(fileset[i], chunks={})\n",
    "ds = ds.expand_dims(time=[time_val])\n",
    "ds = ds.assign_coords(time=(\"time\", [time_val]))\n",
    "matchups = match_points_fast(\n",
    "    ds=ds,\n",
    "    df=df,                      # your surface CHL df\n",
    "    var_name=\"Rrs\",             # or whatever var you're sampling\n",
    "    vector_dim=\"wavelength\",\n",
    "    y_name=\"log10_CHLA_surface\",\n",
    "    ds_lat=\"lat\",\n",
    "    ds_lon=\"lon\",\n",
    "    df_time=\"TIME\",\n",
    "    df_lat=\"LATITUDE\",\n",
    "    df_lon=\"LONGITUDE\",\n",
    ")\n",
    "matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed9518f-4a55-4be7-8a89-6efa275bc1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import xarray as xr\n",
    "\n",
    "def match_points_fast(\n",
    "    ds: xr.Dataset,\n",
    "    df: pd.DataFrame,\n",
    "    var_name: str,\n",
    "    y_name: str,\n",
    "    ds_time: str = \"time\",\n",
    "    ds_lat: str = \"lat\",\n",
    "    ds_lon: str = \"lon\",\n",
    "    df_time: str = \"TIME\",\n",
    "    df_lat: str = \"LATITUDE\",\n",
    "    df_lon: str = \"LONGITUDE\",\n",
    "    dropna_all: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sample variable ds[var_name] at (lat, lon, time) points from df.\n",
    "\n",
    "    ds[var_name] can be either:\n",
    "      - (time, lat, lon)           -> one column named var_name\n",
    "      - (time, lat, lon, extra)   -> multiple columns var_name_<extra>\n",
    "\n",
    "    Returns a DataFrame; if df is empty (after date filtering), returns an empty DF\n",
    "    with proper columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- ensure time dim exists ---\n",
    "    if ds_time not in ds.dims:\n",
    "        tstr = ds.attrs[\"time_coverage_start\"]\n",
    "        t_utc = pd.to_datetime(tstr, utc=True)   # tz-aware Timestamp in UTC\n",
    "        t = t_utc.tz_convert(None)\n",
    "        ds = ds.expand_dims({ds_time: [t]})\n",
    "        ds = ds.assign_coords({ds_time: (ds_time, [t])})\n",
    "\n",
    "    # --- get variable ---\n",
    "    if var_name not in ds:\n",
    "        raise KeyError(f\"{var_name!r} not found in dataset.\")\n",
    "    da_var = ds[var_name]\n",
    "\n",
    "    # --- check dims ---\n",
    "    needed_dims = [ds_time, ds_lat, ds_lon]\n",
    "    if not all(d in da_var.dims for d in needed_dims):\n",
    "        raise ValueError(\n",
    "            f\"{var_name} dims {da_var.dims} do not contain all of {needed_dims}. \"\n",
    "            \"Adjust ds_lat/ds_lon or subset/transform ds[var_name] first.\"\n",
    "        )\n",
    "\n",
    "    # --- normalize to (time, lat, lon) or (time, lat, lon, extra) ---\n",
    "    if da_var.ndim == 3:\n",
    "        # (time, lat, lon)\n",
    "        vector_dim = None\n",
    "        da_var = da_var.transpose(ds_time, ds_lat, ds_lon)\n",
    "    elif da_var.ndim == 4:\n",
    "        # (time, lat, lon, extra)  (extra is always 4th dim)\n",
    "        vector_dim = da_var.dims[3]\n",
    "        da_var = da_var.transpose(ds_time, ds_lat, ds_lon, vector_dim)\n",
    "    else:\n",
    "        raise ValueError(f\"{var_name} has unexpected dims {da_var.dims}\")\n",
    "\n",
    "    # --- coordinate values ---\n",
    "    time_vals = ds[ds_time].values\n",
    "    lat_vals = ds[ds_lat].values\n",
    "    lon_vals = ds[ds_lon].values\n",
    "\n",
    "    if vector_dim is None:\n",
    "        # single “spectral” value per pixel\n",
    "        vec_vals = [None]\n",
    "    else:\n",
    "        vec_vals = ds[vector_dim].values  # array of wavelengths / extra coords\n",
    "\n",
    "    # --- filter df to matching dates ---\n",
    "    t_dates = pd.to_datetime(time_vals).normalize()\n",
    "    df_dates = pd.to_datetime(df[df_time]).dt.normalize()\n",
    "    df2 = df[df_dates.isin(t_dates)].copy()\n",
    "\n",
    "    # If df is empty: return an empty DF with the right columns\n",
    "    if df2.empty:\n",
    "        cols = [df_time, df_lat, df_lon, y_name]\n",
    "        for v in vec_vals:\n",
    "            if v is None:\n",
    "                col_name = var_name\n",
    "            else:\n",
    "                try:\n",
    "                    label = int(v)\n",
    "                except Exception:\n",
    "                    label = v\n",
    "                col_name = f\"{var_name}_{label}\"\n",
    "            cols.append(col_name)\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    # --- Normal path below ---\n",
    "\n",
    "    df_lats = df2[df_lat].to_numpy(dtype=float)\n",
    "    df_lons = df2[df_lon].to_numpy(dtype=float)\n",
    "    df_times = pd.to_datetime(df2[df_time]).values.astype(\"datetime64[ns]\")\n",
    "    y_vals = df2[y_name].to_numpy()\n",
    "\n",
    "    def nearest_index(coord_vals, q):\n",
    "        \"\"\"\n",
    "        Return index of nearest coord_vals for each query q.\n",
    "\n",
    "        coord_vals: 1D array, strictly monotonic (e.g., time, lat, lon)\n",
    "        q         : scalar or 1D array of query values\n",
    "\n",
    "        Returns: 1D array of indices into coord_vals.\n",
    "        \"\"\"\n",
    "        asc = coord_vals[0] <= coord_vals[-1]\n",
    "        base = coord_vals if asc else -coord_vals\n",
    "        tgt = q if asc else -q\n",
    "\n",
    "        idx = np.searchsorted(base, tgt, side=\"left\")\n",
    "        idx0 = np.clip(idx - 1, 0, len(coord_vals) - 1)\n",
    "        idx1 = np.clip(idx,     0, len(coord_vals) - 1)\n",
    "\n",
    "        take_right = np.abs(coord_vals[idx1] - q) < np.abs(coord_vals[idx0] - q)\n",
    "        return np.where(take_right, idx1, idx0)\n",
    "\n",
    "    time_i = nearest_index(time_vals, df_times)\n",
    "    lat_i = nearest_index(lat_vals, df_lats)\n",
    "    lon_i = nearest_index(lon_vals, df_lons)\n",
    "\n",
    "    data_da = da_var.data  # (time, lat, lon) or (time, lat, lon, extra)\n",
    "\n",
    "    if vector_dim is None:\n",
    "        # scalar per pixel: (N_points,)\n",
    "        sampled_da = data_da.vindex[time_i, lat_i, lon_i]\n",
    "    else:\n",
    "        # vector per pixel: (N_points, N_vec)\n",
    "        sampled_da = data_da.vindex[time_i, lat_i, lon_i, :]\n",
    "\n",
    "    if isinstance(sampled_da, da.Array):\n",
    "        sampled = sampled_da.compute()\n",
    "    else:\n",
    "        sampled = np.asarray(sampled_da)\n",
    "\n",
    "    # unify shape: always (N_points, N_vec)\n",
    "    if sampled.ndim == 1:\n",
    "        sampled = sampled[:, None]\n",
    "\n",
    "    # --- build all columns in a dict first ---\n",
    "    data = {\n",
    "        df_time: df_times,\n",
    "        df_lat:  df_lats,\n",
    "        df_lon:  df_lons,\n",
    "        y_name:  y_vals,\n",
    "    }\n",
    "\n",
    "    for j, v in enumerate(vec_vals):\n",
    "        if v is None:\n",
    "            col_name = var_name\n",
    "        else:\n",
    "            try:\n",
    "                label = int(v)\n",
    "            except Exception:\n",
    "                label = v\n",
    "            col_name = f\"{var_name}_{label}\"\n",
    "        data[col_name] = sampled[:, j].astype(float)\n",
    "\n",
    "    out = pd.DataFrame(data)\n",
    "\n",
    "    if dropna_all:\n",
    "        spec_cols = [c for c in out.columns if c.startswith(f\"{var_name}_\") or c == var_name]\n",
    "        mask = np.isfinite(out[spec_cols]).any(axis=1)\n",
    "        out = out[mask].reset_index(drop=True)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab186f4-3b62-45d6-a32b-4328823be06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import xarray as xr\n",
    "\n",
    "def match_points_fast(\n",
    "    ds: xr.Dataset,\n",
    "    df: pd.DataFrame,\n",
    "    var_name: str,\n",
    "    y_name: str,\n",
    "    ds_time: str = \"time\",\n",
    "    ds_lat: str = \"lat\",\n",
    "    ds_lon: str = \"lon\",\n",
    "    df_time: str = \"TIME\",\n",
    "    df_lat: str = \"LATITUDE\",\n",
    "    df_lon: str = \"LONGITUDE\",\n",
    "    dropna_all: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sample variable ds[var_name] at (lat, lon, time) points from df.\n",
    "\n",
    "    ds[var_name] can be either:\n",
    "      - (time, lat, lon)           -> one column named var_name\n",
    "      - (time, lat, lon, extra)   -> multiple columns var_name_<extra>\n",
    "\n",
    "    Returns a DataFrame; if df is empty (after date filtering), returns an empty DF\n",
    "    with proper columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- ensure time dim exists ---\n",
    "    if ds_time not in ds.dims:\n",
    "        tstr = ds.attrs[\"time_coverage_start\"]\n",
    "        t_utc = pd.to_datetime(tstr, utc=True)   # tz-aware Timestamp in UTC\n",
    "        t = t_utc.tz_convert(None)\n",
    "        ds = ds.expand_dims({ds_time: [t]})\n",
    "        ds = ds.assign_coords({ds_time: (ds_time, [t])})\n",
    "\n",
    "    # --- get variable ---\n",
    "    if var_name not in ds:\n",
    "        raise KeyError(f\"{var_name!r} not found in dataset.\")\n",
    "    da_var = ds[var_name]\n",
    "\n",
    "    # --- check dims ---\n",
    "    needed_dims = [ds_time, ds_lat, ds_lon]\n",
    "    if not all(d in da_var.dims for d in needed_dims):\n",
    "        raise ValueError(\n",
    "            f\"{var_name} dims {da_var.dims} do not contain all of {needed_dims}. \"\n",
    "            \"Adjust ds_lat/ds_lon or subset/transform ds[var_name] first.\"\n",
    "        )\n",
    "\n",
    "    # --- normalize to (time, lat, lon) or (time, lat, lon, extra) ---\n",
    "    if da_var.ndim == 3:\n",
    "        # (time, lat, lon)\n",
    "        vector_dim = None\n",
    "        da_var = da_var.transpose(ds_time, ds_lat, ds_lon)\n",
    "    elif da_var.ndim == 4:\n",
    "        # (time, lat, lon, extra)  (extra is always 4th dim)\n",
    "        vector_dim = da_var.dims[3]\n",
    "        da_var = da_var.transpose(ds_time, ds_lat, ds_lon, vector_dim)\n",
    "    else:\n",
    "        raise ValueError(f\"{var_name} has unexpected dims {da_var.dims}\")\n",
    "\n",
    "    if vector_dim is None:        \n",
    "        vec_vals = [None] # single value per pixel\n",
    "    else:\n",
    "        vec_vals = ds[vector_dim].values  # vector of values\n",
    "\n",
    "    # --- filter df to matching dates in ds ---\n",
    "    time_vals = ds[ds_time].values\n",
    "    t_dates = pd.to_datetime(time_vals).normalize()\n",
    "    df_dates = pd.to_datetime(df[df_time]).dt.normalize()\n",
    "    df2 = df[df_dates.isin(t_dates)].copy()\n",
    "\n",
    "    # If df is empty: return an empty DF with the right columns\n",
    "    if df2.empty:\n",
    "        cols = [df_time, df_lat, df_lon, y_name]\n",
    "        for v in vec_vals:\n",
    "            if v is None:\n",
    "                col_name = var_name\n",
    "            else:\n",
    "                try:\n",
    "                    label = int(v)\n",
    "                except Exception:\n",
    "                    label = v\n",
    "                col_name = f\"{var_name}_{label}\"\n",
    "            cols.append(col_name)\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    # for the df with only dates in ds\n",
    "    df_times = pd.to_datetime(df2[df_time]).values.astype(\"datetime64[ns]\")\n",
    "    df_lats = df2[df_lat].to_numpy(dtype=float)\n",
    "    df_lons = df2[df_lon].to_numpy(dtype=float)\n",
    "    y_vals = df2[y_name].to_numpy()\n",
    "\n",
    "    time_idx = ds.get_index(ds_time)\n",
    "    lat_idx = ds.get_index(ds_lat)\n",
    "    lon_idx = ds.get_index(ds_lon)\n",
    "\n",
    "    time_i = time_idx.get_indexer(df_times, method=\"nearest\")\n",
    "    lat_i = lat_idx.get_indexer(df_lats, method=\"nearest\")\n",
    "    lon_i = lon_idx.get_indexer(df_lons, method=\"nearest\")\n",
    "\n",
    "    data_da = da_var.data  # (time, lat, lon) or (time, lat, lon, extra)\n",
    "\n",
    "    if vector_dim is None:\n",
    "        # scalar per pixel: (N_points,)\n",
    "        sampled_da = data_da.vindex[time_i, lat_i, lon_i]\n",
    "    else:\n",
    "        # vector per pixel: (N_points, N_vec)\n",
    "        sampled_da = data_da.vindex[time_i, lat_i, lon_i, :]\n",
    "\n",
    "    if isinstance(sampled_da, da.Array):\n",
    "        sampled = sampled_da.compute()\n",
    "    else:\n",
    "        sampled = np.asarray(sampled_da)\n",
    "\n",
    "    # unify shape: always (N_points, N_vec)\n",
    "    if sampled.ndim == 1:\n",
    "        sampled = sampled[:, None]\n",
    "\n",
    "    # --- build all columns in a dict first ---\n",
    "    data = {\n",
    "        df_time: df_times,\n",
    "        df_lat:  df_lats,\n",
    "        df_lon:  df_lons,\n",
    "        y_name:  y_vals,\n",
    "    }\n",
    "\n",
    "    for j, v in enumerate(vec_vals):\n",
    "        if v is None:\n",
    "            col_name = var_name\n",
    "        else:\n",
    "            try:\n",
    "                label = int(v)\n",
    "            except Exception:\n",
    "                label = v\n",
    "            col_name = f\"{var_name}_{label}\"\n",
    "        data[col_name] = sampled[:, j].astype(float)\n",
    "\n",
    "    out = pd.DataFrame(data)\n",
    "\n",
    "    if dropna_all:\n",
    "        spec_cols = [c for c in out.columns if c.startswith(f\"{var_name}_\") or c == var_name]\n",
    "        mask = np.isfinite(out[spec_cols]).any(axis=1)\n",
    "        out = out[mask].reset_index(drop=True)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c77bf0d-e510-4b62-ba26-ff8b3a0fe643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TIME</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>log10_CHLA_surface</th>\n",
       "      <th>Rrs_346</th>\n",
       "      <th>Rrs_348</th>\n",
       "      <th>Rrs_351</th>\n",
       "      <th>Rrs_353</th>\n",
       "      <th>Rrs_356</th>\n",
       "      <th>Rrs_358</th>\n",
       "      <th>...</th>\n",
       "      <th>Rrs_706</th>\n",
       "      <th>Rrs_707</th>\n",
       "      <th>Rrs_708</th>\n",
       "      <th>Rrs_709</th>\n",
       "      <th>Rrs_711</th>\n",
       "      <th>Rrs_712</th>\n",
       "      <th>Rrs_713</th>\n",
       "      <th>Rrs_714</th>\n",
       "      <th>Rrs_717</th>\n",
       "      <th>Rrs_719</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-05 15:35:00</td>\n",
       "      <td>39.151501</td>\n",
       "      <td>-66.5746</td>\n",
       "      <td>-0.65228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 TIME   LATITUDE  LONGITUDE  log10_CHLA_surface  Rrs_346  \\\n",
       "0 2024-03-05 15:35:00  39.151501   -66.5746            -0.65228      NaN   \n",
       "\n",
       "   Rrs_348  Rrs_351  Rrs_353  Rrs_356  Rrs_358  ...  Rrs_706  Rrs_707  \\\n",
       "0      NaN      NaN      NaN      NaN      NaN  ...      NaN      NaN   \n",
       "\n",
       "   Rrs_708  Rrs_709  Rrs_711  Rrs_712  Rrs_713  Rrs_714  Rrs_717  Rrs_719  \n",
       "0      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
       "\n",
       "[1 rows x 176 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now ds_band[\"Rrs\"] has dims (\"time\", \"lat\", \"lon\")\n",
    "matchups = match_points_fast(\n",
    "    ds=ds,\n",
    "    df=df,                      # your surface CHL df\n",
    "    var_name=\"Rrs\",             # or whatever var you're sampling\n",
    "    vector_dim=\"wavelength\",\n",
    "    y_name=\"log10_CHLA_surface\",\n",
    "    ds_lat=\"lat\",\n",
    "    ds_lon=\"lon\",\n",
    "    df_time=\"TIME\",\n",
    "    df_lat=\"LATITUDE\",\n",
    "    df_lon=\"LONGITUDE\",\n",
    ")\n",
    "matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801038db-6e65-4a86-879f-44916149e94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2024-03-05 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240305.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-06 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240306.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-07 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240307.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-08 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240308.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-09 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240309.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-10 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240310.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-11 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240311.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-12 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240312.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-15 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240315.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-16 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240316.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-17 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240317.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-18 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240318.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-19 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240319.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-20 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240320.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-21 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240321.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-22 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240322.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-23 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240323.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-24 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240324.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-25 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240325.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-28 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240328.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-29 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240329.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-30 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240330.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-31 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240331.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n"
     ]
    }
   ],
   "source": [
    "all_matchups = []\n",
    "\n",
    "for uri, time_val in zip(fileset, fileset_time):\n",
    "    print(f\"Processing {time_val} from {uri}\")\n",
    "\n",
    "    # open with chunks so it's dask-backed, not all in RAM\n",
    "    ds = xr.open_dataset(\n",
    "        uri,\n",
    "        chunks={}  # tweak as needed\n",
    "    )\n",
    "\n",
    "    # add 1-length time dim/coord\n",
    "    ds = ds.expand_dims(time=[time_val])\n",
    "    ds = ds.assign_coords(time=(\"time\", [time_val]))\n",
    "\n",
    "    # call your matcher (it will internally subset df to matching dates)\n",
    "    match_day = match_points_fast(\n",
    "        ds=ds,\n",
    "        df=df,                      # your surface CHL df (all days)\n",
    "        var_name=\"Rrs\",\n",
    "        vector_dim=\"wavelength\",\n",
    "        y_name=\"log10_CHLA_surface\",\n",
    "        ds_lat=\"lat\",\n",
    "        ds_lon=\"lon\",\n",
    "        df_time=\"TIME\",\n",
    "        df_lat=\"LATITUDE\",\n",
    "        df_lon=\"LONGITUDE\",\n",
    "    )\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "    # skip totally empty days to keep concat clean\n",
    "    if not match_day.empty:\n",
    "        all_matchups.append(match_day)\n",
    "\n",
    "# combine everything\n",
    "if all_matchups:\n",
    "    matchups_all = pd.concat(all_matchups, ignore_index=True)\n",
    "else:\n",
    "    # in case literally nothing matched\n",
    "    matchups_all = pd.DataFrame()\n",
    "\n",
    "matchups_all.head()\n",
    "len(matchups_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e82ce-471c-431d-9cb4-1e21773bc076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2024-03-05 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240305.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-06 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240306.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-07 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240307.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-08 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240308.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-09 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240309.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-10 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240310.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-11 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240311.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-12 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240312.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-15 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240315.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-16 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240316.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-17 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240317.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-18 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240318.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-19 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240319.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-20 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240320.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-21 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240321.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-22 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240322.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-23 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240323.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-24 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240324.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-25 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240325.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-28 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240328.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-29 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240329.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-30 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240330.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-31 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240331.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n"
     ]
    }
   ],
   "source": [
    "all_matchups = []\n",
    "\n",
    "for uri, time_val in zip(fileset, fileset_time):\n",
    "    print(f\"Processing {time_val} from {uri}\")\n",
    "\n",
    "    # Open lazily, don’t cache the file\n",
    "    with xr.open_dataset(\n",
    "        uri,\n",
    "        chunks={},\n",
    "        cache=False,\n",
    "    ) as ds_raw:\n",
    "\n",
    "        # Add 1-length time dimension\n",
    "        ds = ds_raw.expand_dims(time=[time_val])\n",
    "        ds = ds.assign_coords(time=(\"time\", [time_val]))\n",
    "\n",
    "        match_day = match_points_fast(\n",
    "            ds=ds,\n",
    "            df=df,                      # full Argo df; function filters by date\n",
    "            var_name=\"Rrs\",\n",
    "            vector_dim=\"wavelength\",\n",
    "            y_name=\"log10_CHLA_surface\",\n",
    "            ds_lat=\"lat\",\n",
    "            ds_lon=\"lon\",\n",
    "            df_time=\"TIME\",\n",
    "            df_lat=\"LATITUDE\",\n",
    "            df_lon=\"LONGITUDE\",\n",
    "        )\n",
    "    \n",
    "    # be explicit about cleaning up\n",
    "    del ds_raw\n",
    "    gc.collect()\n",
    "\n",
    "    if not match_day.empty:\n",
    "        all_matchups.append(match_day)\n",
    "\n",
    "# Combine all days into one DF\n",
    "if all_matchups:\n",
    "    matchups_all = pd.concat(all_matchups, ignore_index=True)\n",
    "else:\n",
    "    matchups_all = pd.DataFrame()\n",
    "\n",
    "print(len(matchups_all))\n",
    "matchups_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f950cb90-18c2-45c4-b027-7552f3699505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2024-03-05 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240305.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-06 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240306.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-07 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240307.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-08 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240308.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-09 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240309.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-10 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240310.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-11 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240311.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-12 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240312.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-15 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240315.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-16 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240316.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-17 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240317.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-18 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240318.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-19 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240319.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-20 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240320.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-21 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240321.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-22 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240322.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-23 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240323.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-24 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240324.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-25 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240325.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-28 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240328.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-29 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240329.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-30 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240330.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-03-31 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240331.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-04-01 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240401.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-04-02 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240402.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-04-03 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240403.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-04-04 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240404.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-04-05 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240405.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-04-06 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240406.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n",
      "Processing 2024-04-07 from <File-like object S3FileSystem, ob-cumulus-prod-public/PACE_OCI.20240407.L3m.DAY.RRS.V3_1.Rrs.4km.nc>\n"
     ]
    }
   ],
   "source": [
    "for uri, time_val in zip(fileset, fileset_time):\n",
    "    print(f\"Processing {time_val} from {uri}\")\n",
    "\n",
    "    # open with chunks so it's dask-backed, not all in RAM\n",
    "    ds = xr.open_dataset(\n",
    "        uri,\n",
    "        chunks={}  # tweak as needed\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d01c85e8-b8e6-4f44-aa7f-380fa095a8c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mds\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b57959-ea0f-4bb2-8555-564d2cc07eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
